{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b7dd4a4",
   "metadata": {},
   "source": [
    "## Enviroment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe56abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6be052d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nurja\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import NeighborSampler\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(rc={'figure.figsize':(16.7,8.27)})\n",
    "sns.set_theme(style=\"ticks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9eb6b6",
   "metadata": {},
   "source": [
    "### Dataset explorartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b90628c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found. Loading without downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nurja\\anaconda3\\lib\\site-packages\\torch_geometric\\data\\dataset.py:189: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "c:\\Users\\nurja\\anaconda3\\lib\\site-packages\\torch_geometric\\data\\dataset.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "c:\\Users\\nurja\\anaconda3\\lib\\site-packages\\ogb\\nodeproppred\\dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Step 2: Define dataset path\n",
    "root = 'home/data/products'  # You can change this to your desired path\n",
    "\n",
    "# Step 3: Check if dataset folder exists\n",
    "if not os.path.exists(root):\n",
    "    print(\"Dataset folder not found! Downloading now...\")\n",
    "    # Initialize dataset object, this will trigger download automatically\n",
    "    dataset = PygNodePropPredDataset(name='ogbn-products', root=root)\n",
    "    print(\"Dataset downloaded and loaded successfully.\")\n",
    "else:\n",
    "    print(\"Dataset found. Loading without downloading...\")\n",
    "    dataset = PygNodePropPredDataset(name='ogbn-products', root=root)\n",
    "    print(\"Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e17bc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 2449029\n",
      "Number of edges: 123718280\n",
      "Node feature shape: torch.Size([2449029, 100])\n",
      "Edge index shape: torch.Size([2, 123718280])\n",
      "Number of classes: 47\n"
     ]
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "split_idx = dataset.get_idx_split()\n",
    "evaluator = Evaluator(name='ogbn-products')\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Node feature shape: {data.x.shape}\")\n",
    "print(f\"Edge index shape: {data.edge_index.shape}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9174a",
   "metadata": {},
   "source": [
    "## Label Mapping and Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf046a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label idx</th>\n",
       "      <th>product category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Home &amp; Kitchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sports &amp; Outdoors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Patio, Lawn &amp; Garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>CDs &amp; Vinyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Cell Phones &amp; Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Grocery &amp; Gourmet Food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label idx           product category\n",
       "0          0             Home & Kitchen\n",
       "1          1     Health & Personal Care\n",
       "2          2                     Beauty\n",
       "3          3          Sports & Outdoors\n",
       "4          4                      Books\n",
       "5          5       Patio, Lawn & Garden\n",
       "6          6               Toys & Games\n",
       "7          7                CDs & Vinyl\n",
       "8          8  Cell Phones & Accessories\n",
       "9          9     Grocery & Gourmet Food"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('home/data/products/ogbn_products/mapping/labelidx2productcategory.csv.gz')\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11d2c849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts: Counter({4: 668950, 7: 172199, 6: 158771, 3: 151061, 12: 131886, 2: 116043, 0: 114294, 8: 110796, 1: 109832, 13: 101541, 16: 83594, 21: 80795, 9: 67358, 10: 52345, 18: 49019, 24: 45406, 17: 42337, 5: 40715, 11: 32937, 42: 32500, 15: 26911, 20: 22575, 19: 17438, 23: 3653, 14: 3079, 25: 3024, 28: 1969, 29: 1561, 43: 1399, 22: 879, 36: 630, 44: 566, 26: 553, 37: 514, 32: 513, 31: 418, 30: 277, 27: 259, 34: 154, 38: 91, 41: 61, 35: 44, 39: 37, 33: 29, 45: 9, 40: 6, 46: 1})\n"
     ]
    }
   ],
   "source": [
    "# Load label mapping\n",
    "mapping_file = osp.join(root, 'ogbn_products', 'mapping', 'labelidx2productcategory.csv.gz')\n",
    "df = pd.read_csv(mapping_file)\n",
    "\n",
    "# Create label mapping dictionary\n",
    "label_mapping = dict(zip(df.iloc[:, 0], df.iloc[:, 1]))\n",
    "\n",
    "# Check distribution\n",
    "y = data.y.squeeze().tolist()\n",
    "label_counts = collections.Counter(y)\n",
    "print(\"Label counts:\", label_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986716dd",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28a3bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Build SAGE convolution layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Optional: Reset parameters (good for re-initializing before retraining)\"\"\"\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        \"\"\"Forward used during training with neighbor sampling.\"\"\"\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def inference(self, x_all, subgraph_loader, device):\n",
    "   \n",
    "        pbar = tqdm(total=x_all.size(0) * self.num_layers)\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                edge_index, _, size = adj  # unpack first\n",
    "                edge_index = edge_index.to(device)  # move edge_index to device\n",
    "\n",
    "                x = x_all[n_id].to(device)  # move node features to device\n",
    "                x_target = x[:size[1]]  # get target nodes\n",
    "                x = self.convs[i]((x, x_target), edge_index)\n",
    "\n",
    "                if i != self.num_layers - 1:\n",
    "                    x = F.relu(x)\n",
    "\n",
    "                xs.append(x)\n",
    "                pbar.update(batch_size)\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "\n",
    "        pbar.close()\n",
    "        return x_all\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1044a2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e8272a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nurja\\anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.NeighborSampler' is deprecated, use 'loader.NeighborSampler' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Loss: 0.6443, Train Accuracy: 0.8314\n",
      "Epoch 02, Loss: 0.3871, Train Accuracy: 0.8931\n",
      "Epoch 03, Loss: 0.3490, Train Accuracy: 0.9029\n",
      "Epoch 04, Loss: 0.3347, Train Accuracy: 0.9068\n",
      "Epoch 05, Loss: 0.3162, Train Accuracy: 0.9111\n",
      "Epoch 06, Loss: 0.3102, Train Accuracy: 0.9114\n",
      "Epoch 07, Loss: 0.3034, Train Accuracy: 0.9138\n",
      "Epoch 08, Loss: 0.2986, Train Accuracy: 0.9160\n",
      "Epoch 09, Loss: 0.3006, Train Accuracy: 0.9147\n",
      "Epoch 10, Loss: 0.2945, Train Accuracy: 0.9161\n",
      "Epoch 11, Loss: 0.2895, Train Accuracy: 0.9175\n",
      "Epoch 12, Loss: 0.2879, Train Accuracy: 0.9188\n",
      "Epoch 13, Loss: 0.3004, Train Accuracy: 0.9153\n",
      "Epoch 14, Loss: 0.2829, Train Accuracy: 0.9195\n",
      "Epoch 15, Loss: 0.2792, Train Accuracy: 0.9204\n",
      "Epoch 16, Loss: 0.2780, Train Accuracy: 0.9204\n",
      "Epoch 17, Loss: 0.2836, Train Accuracy: 0.9193\n",
      "Epoch 18, Loss: 0.2764, Train Accuracy: 0.9207\n",
      "Epoch 19, Loss: 0.2728, Train Accuracy: 0.9215\n",
      "Epoch 20, Loss: 0.2711, Train Accuracy: 0.9219\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphSAGE(dataset.num_features, 256, dataset.num_classes, num_layers=3).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "x = data.x.to(device)\n",
    "y = data.y.squeeze().to(device)\n",
    "\n",
    "# NeighborSampler for mini-batch training\n",
    "train_loader = NeighborSampler(data.edge_index, node_idx=split_idx['train'],\n",
    "                                sizes=[15, 10, 5], batch_size=1024, shuffle=True)\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    total_loss = total_correct = 0\n",
    "    for batch_size, n_id, adjs in loader:\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x[n_id], adjs)\n",
    "        out = out.log_softmax(dim=-1)\n",
    "        loss = F.nll_loss(out, y[n_id[:batch_size]])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\n",
    "\n",
    "    return total_loss / len(loader), total_correct / split_idx['train'].size(0)\n",
    "\n",
    "# Train for 20 epochs\n",
    "for epoch in range(1, 21):\n",
    "    loss, acc = train(model, train_loader)\n",
    "    print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Train Accuracy: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b1ac0",
   "metadata": {},
   "source": [
    "## Save and reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac3ac353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nurja\\AppData\\Local\\Temp\\ipykernel_9844\\1051796167.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('graphsage_trained_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), 'graphsage_trained_model.pth')\n",
    "\n",
    "# Later: reload model\n",
    "model.load_state_dict(torch.load('graphsage_trained_model.pth'))\n",
    "model = model.to(device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
